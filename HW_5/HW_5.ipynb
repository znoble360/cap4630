{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9aCdtizZVtrsLOInrKp2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/znoble360/cap4630/blob/master/HW_5/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO6XKR7zzYMu",
        "colab_type": "text"
      },
      "source": [
        "#1. General concepts (for instance, what are artificial intelligence, machine learning, and deep learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeBhEaB0gy4",
        "colab_type": "text"
      },
      "source": [
        "In this class, I have learned that artificial intelligence can take many different forms. One of the simplest forms that A.I. can take is simple condition statements, manually added in by the software developer. I have learned that one of the most prominent, promising, and improving forms of A.I. is machine learning, and one prominent form of machine learning is deep learning. <br>\n",
        "<br>\n",
        "Symbolic A.I. (or GOFAI) is programed explicitly by the software developer and consists mainly of conditional statements. This type of A.I. is limited to what situations the software developer can think of and may miss some conditions that might be helpful in the A.I. being able to do its job more effectively.\n",
        "<br>\n",
        "<br>\n",
        "Deep learning is a form of A.I. that the software developer does not explicitly write themself, but instead designs a model for the A.I. to train itself. A software developer that wishes to implement deep learning must design a model for the A.I., then give the model large amounts of data in order for the A.I. model to start to learn how to make decisions better. Decisions that deep learning models can make might include estimating the probability of one thing to happen or not, the probability that an input is one thing vs one other thing, or it could classify an input as one of multiple different classes with a certain confidence level for each classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHC8brrHzX8k",
        "colab_type": "text"
      },
      "source": [
        "#2. Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIv6aFua0hn-",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is a type of deep learning A.I. that attempts to find an n-dimensional plane of best fit for a given set of data. This plane of best fit is then used for predicting new values for new data. This plane of best fit is the model. The model can take a number of input parameters, and this number of input parameters determines n, the number of dimensions the plane of best fit will be. When given all the input parameters for a given data sample, the model predicts an output value that would ideally match best with the previous data given to train the model.\n",
        "<br>\n",
        "<br>\n",
        "The inner workings of the model are involved with matrix math. The model is just a collection of matrices filled with weights and biases. In order for the model to make a prediction given a data sample, is the data sample must be in the form of an array, and then the model multiplies the data sample matrix through the matrix(es) filled with weights and biases to get an output value on the other side.\n",
        "<br>\n",
        "<br>\n",
        "In order to train this model to be effective at predicting an output value for a given data sample, the model must use weights and biases, and update these incrementally using training data where the model can be given a data sample with a known output value. When the model is given a data sample, it must predict an output value, then adjust the weights and biases based on the prediction and how far it is from the actual known output value.\n",
        "<br>\n",
        "<br>\n",
        "The way the model updates the values of the weights and biases is by using 2 concepts: gradients and gradient decent, along with some form of loss function. A gradient is the rate of change in a function at a given value. Gradient decent is where you move in the direction of the negative gradient. \n",
        "<br>\n",
        "<br>\n",
        "The loss function is some function that can model how far each prediction is from the actual value it should be. Gradient decent is used to minimize the loss function so that the predictions are as close as possible to the actual value.\n",
        "<br>\n",
        "<br>\n",
        "Logistic regression is similar in concept to linear regression when it comes to training and predictions. They both use gradient decent, loss functions, matrices, weights and biases, and training data. The biggest difference between linear and logistic regression is the equations used for the loss functions,  gradient descent, and the use case for logistic regression. While linear regression is used to predict a certain output value, logistic regression is used to classify data samples into discrete classes, whether it be 2, 3, or more different classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ty7GE-zX2g",
        "colab_type": "text"
      },
      "source": [
        "#3. Building a model (for instance, here you can talk about the structure of a convent, what it's components are etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jShAM6QX0iEs",
        "colab_type": "text"
      },
      "source": [
        "A convolutional network (aka convnet) is a form of deep learning that is better suited for classification of images, since there is no simple way to distil meaningful data values that would be helpful in classification from an image. A convnet consists of two main sections: the convnet base, and the classifier.\n",
        "<br>\n",
        "<br>\n",
        "The classifier is made up of some number of densely connected layers that take the output of the convnet base and make a prediction for what classification the data should be under. This is what you would train most importantly if you were going to use a pretrained convnet base.\n",
        "<br>\n",
        "<br>\n",
        "The convnet base is the part of the model that distils computer readable data from an image. This is done by convolution/correlation. A number of filters are applied to every possible position of the image. These filters in the lowest levels of the base associate small patterns in pixels like diagonal lines or horizontal lines. As the data passes through more layers of the convnet base, the filters start piecing together more and more abstract ideas, like putting together two horizontal lines and two vertical lines all together makes a square. The lines would be the most basic layer of filters, and the next layer of filters would piece together the lines to see a square. the more filters the data passes through, the bigger of a concept can become, until it reaches the end and eventually goes through the classifier. \n",
        "<br>\n",
        "<br>\n",
        "One perhaps unintended effect of the filters is that using several filters in a layer will greatly increase the number of parameters in the model, making training take great amounts of time. One way to counter this effect is pooling. A common form of pooling is maxpooling. Maxpooling is where the number of parameters is condensed by only taking the max value of sub-sections of the image/filter layer. These sub-sections of the image are typically a 2-by-2 grid of pixels but can be any other size as well. These \"sub-sections\" are called windows. The result of maxpooling is a much smaller, more manageable number of parameters in the model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ_CydywwxPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of maxpooling:\n",
        "#  --------------------------------\n",
        "# |       |       ||       |       |                 window of size 2x2\n",
        "# |   4   |   1   ||   5   |   4   |                 image  of size 4x4\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   2   ||   4   |   2   |\n",
        "# |       |       ||       |       |\n",
        "# |===============||===============|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   6   |   6   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   1   |   0   |\n",
        "# |       |       ||       |       |\n",
        "#  --------------------------------\n",
        "#\n",
        "#  --------------------------------\n",
        "# |       |       ||       |       |\n",
        "# |  *4*  |   1   ||  *5*  |   4   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|            divided into 2x2 windows\n",
        "# |       |       ||       |       |            find the maximum in each window, then keep only that\n",
        "# |   1   |   2   ||   4   |   2   |\n",
        "# |       |       ||       |       |\n",
        "# |===============||===============|\n",
        "# |       |       ||       |       |\n",
        "# |  *1*  |   0   ||  *6*  |   6   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   1   |   0   |\n",
        "# |       |       ||       |       |\n",
        "#  --------------------------------\n",
        "#\n",
        "#  --------------------------------\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |       4       ||       5       | \n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |===============||===============|\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |       1       ||       6       | \n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "#  --------------------------------\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDIGzOJAzXwV",
        "colab_type": "text"
      },
      "source": [
        "#4. Comping a model (for instance, you can talk here about learning rate etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zYfXjkw0ihz",
        "colab_type": "text"
      },
      "source": [
        "When training a machine learning model and updating the weights and biases to make its predictions better, a learning rate should be chosen. This learning rate determines how much each weight and bias should be changed each update to the model. If the learning rate is too high, the model might over correct every time the weights and biases are updated. If the learning rate is too low, then the model will take much longer to train, and might take too long to be practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kllv_bdzXpo",
        "colab_type": "text"
      },
      "source": [
        "#5. Training a model (for instance, you can talk about overfitting/underfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2wB4FNy0jEY",
        "colab_type": "text"
      },
      "source": [
        "When training a machine learning model, there are two extremes that should be avoided if possible: overfitting and underfitting.\n",
        "<br>\n",
        "<br>\n",
        "Overfitting occurs most typically when there is too little data or the model being used is too complicated. Overfitting is when a model trains on a set of data so well that it becomes less a less generalized model. The more the model trains on the training data, the model will start to lose the ability to predict outputs for inputs it has not seen, and will be more prone to fail edge cases.\n",
        "<br>\n",
        "<br>\n",
        "Underfitting is the opposite extreme, where either the model for the A.I. is too simple, or the model has not trained on enough data. An underfitted model will likely be less accurate than an overfitted model because an overfitted model can at least accurately predict the training data. Underfitted data will not be able to guess very well because the model would not have trained enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ibialF3zXZA",
        "colab_type": "text"
      },
      "source": [
        "#6. Finetuning a pretrained model (describe how you proceed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_NoCDF0j6u",
        "colab_type": "text"
      },
      "source": [
        "The process of finetuning a pretrained model has several steps. \n",
        "<br>\n",
        "<br>\n",
        "The first step is finding and downloading/importing a pre-trained model. Then be sure to freeze all of the chosen convnet model's layers so as to not lose the pretrained part of the model. After this, one must design a classifier to classify the output of the convnet base. Once this is done, the classifier must be attached to the convnet base in order to train the newly created classifier. Once the classifier has been trained sufficiently without overfitting, a small handful of shallow layers of the convnet base should be unfrozen and then the whole thing should be trained again in order to adapt the base to the classifier. Once training is finished, check that overfitting did not occur. After this, the various model parameters may be tweaked for possible performance improvement, model parameters being things like epochs, size of epochs, number of nodes, learning rate, etc. Other things to adjust could be trying to unfreeze different layers than the ones that were originally unfrozen and adding some data augmentation to your data pool for training. \n",
        "<br>\n",
        "<br>\n",
        "Data augmentation involves altering data so that it is almost like it is a different datum, but still classifies the same as the source. Ideally this data should still have very similar characteristics to its source, but just enough change to be different."
      ]
    }
  ]
}