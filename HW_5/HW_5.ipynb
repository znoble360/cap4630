{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4cZscK9qSBFidi1rKCYae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/znoble360/cap4630/blob/master/HW_5/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO6XKR7zzYMu",
        "colab_type": "text"
      },
      "source": [
        "#1. General concepts (for instance, what are artificial intelligence, machine learning, and deep learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeBhEaB0gy4",
        "colab_type": "text"
      },
      "source": [
        "In this class, I have learned that artificial intelligence can take many different forms. One of the simplest forms that A.I. can take is simple condition statements, manually added in by the software developer. I have learned that one of the most prominent, promising, and improving forms of A.I. is machine learning, and one prominent form of machine learning is deep learning. <br>\n",
        "<br>\n",
        "Symbolic A.I. (or GOFAI) is programed explicitly by the software developer and consists mainly of conditional statements. This type of A.I. is limited to what situations the software developer can think of and may miss some conditions that might be helpful in the A.I. being able to do its job more effectively.\n",
        "<br>\n",
        "<br>\n",
        "Deep learning is a form of A.I. that the software developer does not explicitly write themself, but instead designs a model for the A.I. to train itself. A software developer that wishes to implement deep learning must design a model for the A.I., then give the model large amounts of data in order for the A.I. model to start to learn how to make decisions better. Decisions that deep learning models can make might include estimating the probability of one thing to happen or not, the probability that an input is one thing vs one other thing, or it could classify an input as one of multiple different classes with a certain confidence level for each classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHC8brrHzX8k",
        "colab_type": "text"
      },
      "source": [
        "#2. Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIv6aFua0hn-",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is a type of deep learning A.I. that attempts to find an n-dimensional plane of best fit for a given set of data. This plane of best fit is then used for predicting new values for new data. This plane of best fit is the model. The model can take a number of input parameters, and this number of input parameters determines n, the number of dimensions the plane of best fit will be. When given all the input parameters for a given data sample, the model predicts an output value that would ideally match best with the previous data given to train the model.\n",
        "<br>\n",
        "<br>\n",
        "The inner workings of the model are involved with matrix math. The model is just a collection of matrices filled with weights and biases. In order for the model to make a prediction given a data sample, is the data sample must be in the form of an array, and then the model multiplies the data sample matrix through the matrix(es) filled with weights and biases to get an output value on the other side.\n",
        "<br>\n",
        "<br>\n",
        "In order to train this model to be effective at predicting an output value for a given data sample, the model must use weights and biases, and update these incrementally using training data where the model can be given a data sample with a known output value. When the model is given a data sample, it must predict an output value, then adjust the weights and biases based on the prediction and how far it is from the actual known output value.\n",
        "<br>\n",
        "<br>\n",
        "The way the model updates the values of the weights and biases is by using 2 concepts: gradients and gradient decent, along with some form of loss function. A gradient is the rate of change in a function at a given value. Gradient decent is where you move in the direction of the negative gradient. \n",
        "<br>\n",
        "<br>\n",
        "The loss function is some function that can model how far each prediction is from the actual value it should be. Gradient decent is used to minimize the loss function so that the predictions are as close as possible to the actual value.\n",
        "<br>\n",
        "<br>\n",
        "Logistic regression is similar in concept to linear regression when it comes to training and predictions. They both use gradient decent, loss functions, matrices, weights and biases, and training data. The biggest difference between linear and logistic regression is the equations used for the loss functions,  gradient descent, and the use case for logistic regression. While linear regression is used to predict a certain output value, logistic regression is used to classify data samples into discrete classes, whether it be 2, 3, or more different classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxIPDZIfXAAX",
        "colab_type": "text"
      },
      "source": [
        "##Code examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktoEI1m3TXgc",
        "colab_type": "text"
      },
      "source": [
        "###Linear regression code example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJyjTTnTW-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate m  partialy random triples (x, y, z)\n",
        "\n",
        "m = 100\n",
        "X1 = 2 * np.random.rand(m, 1) - 1\n",
        "X2 = 2 * np.random.rand(m, 1) - 1\n",
        "Y = 3 * X1 + np.random.rand(m, 1)\n",
        "\n",
        "# add x0 = 1 \n",
        "X_b = np.column_stack([np.ones((m, 1)), X1, X2])\n",
        "\n",
        "# create array with a 3d x outlining the plotted area\n",
        "X_new = np.array([[0, 0], \n",
        "                  [1, 1], \n",
        "                  [-1, -1], \n",
        "                  [-1, 1], \n",
        "                  [1, -1]])\n",
        "\n",
        "# add x0 = 1 to each instance\n",
        "X_new_b = np.column_stack([np.ones((5, 1)), X_new])\n",
        "\n",
        "# number of epochs\n",
        "epochs = 20\n",
        "# learning rate\n",
        "lr = 0.01\n",
        "# fix initial random weight for gradient descent (batch gradient descent)\n",
        "np.random.seed(42)\n",
        "initial_weight = np.random.randn(3, 1) \n",
        "\n",
        "weight = initial_weight\n",
        "batch_size = 4\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = Y[shuffled_indices]\n",
        "    for i in range(0, m, batch_size):\n",
        "        xi = X_b_shuffled[i:i+batch_size]\n",
        "        yi = y_shuffled[i:i+batch_size]\n",
        "        gradient = 1 / batch_size * xi.T.dot(xi.dot(weight) - yi)\n",
        "        weight = weight - lr * gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp8aZTiZUtv8",
        "colab_type": "text"
      },
      "source": [
        "###Logistic regression code example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3KcuzWVEPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e570863d-0dc1-4744-ec03-9b4ef6bad50b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "w = 1\n",
        "b = 2\n",
        "mu = 1\n",
        "sigma = 1\n",
        "m = 1000\n",
        "num_epochs = 100\n",
        "num_batch_size = int(m*0.8/40)\n",
        "lr = 0.001\n",
        "\n",
        "\n",
        "def get_random_data(w, b, mu, sigma, m):\n",
        "    \n",
        "    x_1 = np.random.uniform(0, 1, m)\n",
        "    x_2 = np.empty([m])\n",
        "    labels = np.empty([m])\n",
        "\n",
        "    # generate x_2 values\n",
        "    for i in range(m):\n",
        "        c = np.random.randint(2)\n",
        "        n = np.random.normal(mu, sigma, m)\n",
        "        val = (w * x_1[i]) + b + (((-1) ** c) * n[i])\n",
        "\n",
        "        x_2[i] = val\n",
        "        labels[i] = c\n",
        "\n",
        "    data = np.stack((x_1, x_2))\n",
        "\n",
        "    return data.T, labels.T\n",
        "\n",
        "training_size = int(m * 0.8)\n",
        "test_size = m - training_size\n",
        "\n",
        "data, labels = get_random_data(w, b, mu, sigma, m)\n",
        "\n",
        "split = np.split(data, [training_size])\n",
        "training_data = split[0]\n",
        "test_data = split[1]\n",
        "\n",
        "split = np.split(labels, [training_size])\n",
        "training_labels = split[0]\n",
        "test_labels = split[1]\n",
        "\n",
        "def build_and_compile_model():\n",
        "    # build model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    #model.add(tf.keras.layers.Dense(2, activation='relu', input_shape=(2,)))\n",
        "    #model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "    #model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(2,)))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = build_and_compile_model()\n",
        "\n",
        "history = model.fit(training_data,\n",
        "                    training_labels,\n",
        "                    epochs=num_epochs,\n",
        "                    batch_size=num_batch_size,\n",
        "                    validation_data=(test_data, test_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "40/40 [==============================] - 0s 5ms/step - loss: 1.1579 - accuracy: 0.5175 - val_loss: 1.1680 - val_accuracy: 0.4850\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 1.1072 - accuracy: 0.5213 - val_loss: 1.1178 - val_accuracy: 0.4850\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 1.0593 - accuracy: 0.5238 - val_loss: 1.0684 - val_accuracy: 0.4850\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 1.0131 - accuracy: 0.5238 - val_loss: 1.0213 - val_accuracy: 0.4950\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.9680 - accuracy: 0.5275 - val_loss: 0.9744 - val_accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.9246 - accuracy: 0.5288 - val_loss: 0.9294 - val_accuracy: 0.5050\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.8807 - accuracy: 0.5300 - val_loss: 0.8847 - val_accuracy: 0.5050\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.8400 - accuracy: 0.5350 - val_loss: 0.8435 - val_accuracy: 0.5050\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.8013 - accuracy: 0.5375 - val_loss: 0.8034 - val_accuracy: 0.5050\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.7641 - accuracy: 0.5437 - val_loss: 0.7657 - val_accuracy: 0.5150\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.7298 - accuracy: 0.5525 - val_loss: 0.7308 - val_accuracy: 0.5250\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6981 - accuracy: 0.5612 - val_loss: 0.6981 - val_accuracy: 0.5400\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.5725 - val_loss: 0.6695 - val_accuracy: 0.5750\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.5850 - val_loss: 0.6437 - val_accuracy: 0.5800\n",
            "Epoch 15/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.5975 - val_loss: 0.6222 - val_accuracy: 0.5850\n",
            "Epoch 16/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6048 - accuracy: 0.6150 - val_loss: 0.6022 - val_accuracy: 0.6100\n",
            "Epoch 17/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.6388 - val_loss: 0.5868 - val_accuracy: 0.6350\n",
            "Epoch 18/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.6513 - val_loss: 0.5747 - val_accuracy: 0.6700\n",
            "Epoch 19/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.6662 - val_loss: 0.5640 - val_accuracy: 0.6800\n",
            "Epoch 20/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.6825 - val_loss: 0.5558 - val_accuracy: 0.7100\n",
            "Epoch 21/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.7050 - val_loss: 0.5479 - val_accuracy: 0.7300\n",
            "Epoch 22/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7262 - val_loss: 0.5408 - val_accuracy: 0.7450\n",
            "Epoch 23/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7425 - val_loss: 0.5342 - val_accuracy: 0.7500\n",
            "Epoch 24/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7437 - val_loss: 0.5282 - val_accuracy: 0.7750\n",
            "Epoch 25/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7513 - val_loss: 0.5218 - val_accuracy: 0.7850\n",
            "Epoch 26/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7650 - val_loss: 0.5161 - val_accuracy: 0.7850\n",
            "Epoch 27/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7688 - val_loss: 0.5103 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7763 - val_loss: 0.5043 - val_accuracy: 0.8000\n",
            "Epoch 29/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7800 - val_loss: 0.4995 - val_accuracy: 0.8050\n",
            "Epoch 30/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7862 - val_loss: 0.4948 - val_accuracy: 0.8050\n",
            "Epoch 31/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7900 - val_loss: 0.4897 - val_accuracy: 0.8200\n",
            "Epoch 32/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7962 - val_loss: 0.4851 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.8012 - val_loss: 0.4808 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8075 - val_loss: 0.4765 - val_accuracy: 0.8200\n",
            "Epoch 35/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8087 - val_loss: 0.4727 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.8138 - val_loss: 0.4681 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8175 - val_loss: 0.4640 - val_accuracy: 0.8250\n",
            "Epoch 38/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.8188 - val_loss: 0.4594 - val_accuracy: 0.8350\n",
            "Epoch 39/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8188 - val_loss: 0.4553 - val_accuracy: 0.8350\n",
            "Epoch 40/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8238 - val_loss: 0.4519 - val_accuracy: 0.8350\n",
            "Epoch 41/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.8263 - val_loss: 0.4486 - val_accuracy: 0.8350\n",
            "Epoch 42/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.8275 - val_loss: 0.4450 - val_accuracy: 0.8400\n",
            "Epoch 43/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8300 - val_loss: 0.4418 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.8288 - val_loss: 0.4384 - val_accuracy: 0.8350\n",
            "Epoch 45/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.8300 - val_loss: 0.4354 - val_accuracy: 0.8350\n",
            "Epoch 46/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.8325 - val_loss: 0.4318 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.8338 - val_loss: 0.4285 - val_accuracy: 0.8450\n",
            "Epoch 48/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.8363 - val_loss: 0.4251 - val_accuracy: 0.8650\n",
            "Epoch 49/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8363 - val_loss: 0.4220 - val_accuracy: 0.8700\n",
            "Epoch 50/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8350 - val_loss: 0.4188 - val_accuracy: 0.8700\n",
            "Epoch 51/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8375 - val_loss: 0.4157 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8375 - val_loss: 0.4127 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8363 - val_loss: 0.4097 - val_accuracy: 0.8700\n",
            "Epoch 54/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8388 - val_loss: 0.4071 - val_accuracy: 0.8700\n",
            "Epoch 55/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8388 - val_loss: 0.4049 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.8413 - val_loss: 0.4023 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8400 - val_loss: 0.3997 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8400 - val_loss: 0.3973 - val_accuracy: 0.8800\n",
            "Epoch 59/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8400 - val_loss: 0.3948 - val_accuracy: 0.8800\n",
            "Epoch 60/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8388 - val_loss: 0.3926 - val_accuracy: 0.8800\n",
            "Epoch 61/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8400 - val_loss: 0.3899 - val_accuracy: 0.8900\n",
            "Epoch 62/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8425 - val_loss: 0.3877 - val_accuracy: 0.8900\n",
            "Epoch 63/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8400 - val_loss: 0.3857 - val_accuracy: 0.8900\n",
            "Epoch 64/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8400 - val_loss: 0.3839 - val_accuracy: 0.8900\n",
            "Epoch 65/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8425 - val_loss: 0.3818 - val_accuracy: 0.8900\n",
            "Epoch 66/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8425 - val_loss: 0.3798 - val_accuracy: 0.8900\n",
            "Epoch 67/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8438 - val_loss: 0.3777 - val_accuracy: 0.8950\n",
            "Epoch 68/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8438 - val_loss: 0.3761 - val_accuracy: 0.8950\n",
            "Epoch 69/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8413 - val_loss: 0.3741 - val_accuracy: 0.8950\n",
            "Epoch 70/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8438 - val_loss: 0.3724 - val_accuracy: 0.8950\n",
            "Epoch 71/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8450 - val_loss: 0.3706 - val_accuracy: 0.8950\n",
            "Epoch 72/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8462 - val_loss: 0.3690 - val_accuracy: 0.8900\n",
            "Epoch 73/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8450 - val_loss: 0.3673 - val_accuracy: 0.8900\n",
            "Epoch 74/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8462 - val_loss: 0.3657 - val_accuracy: 0.8900\n",
            "Epoch 75/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8475 - val_loss: 0.3639 - val_accuracy: 0.8900\n",
            "Epoch 76/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8475 - val_loss: 0.3623 - val_accuracy: 0.8950\n",
            "Epoch 77/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8475 - val_loss: 0.3607 - val_accuracy: 0.8950\n",
            "Epoch 78/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8487 - val_loss: 0.3592 - val_accuracy: 0.8950\n",
            "Epoch 79/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8487 - val_loss: 0.3578 - val_accuracy: 0.8950\n",
            "Epoch 80/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8487 - val_loss: 0.3564 - val_accuracy: 0.9000\n",
            "Epoch 81/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8500 - val_loss: 0.3550 - val_accuracy: 0.9000\n",
            "Epoch 82/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8500 - val_loss: 0.3538 - val_accuracy: 0.9000\n",
            "Epoch 83/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8525 - val_loss: 0.3525 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8500 - val_loss: 0.3512 - val_accuracy: 0.9000\n",
            "Epoch 85/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8500 - val_loss: 0.3499 - val_accuracy: 0.9000\n",
            "Epoch 86/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8512 - val_loss: 0.3488 - val_accuracy: 0.9000\n",
            "Epoch 87/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8525 - val_loss: 0.3476 - val_accuracy: 0.9000\n",
            "Epoch 88/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8525 - val_loss: 0.3465 - val_accuracy: 0.9000\n",
            "Epoch 89/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8512 - val_loss: 0.3453 - val_accuracy: 0.9000\n",
            "Epoch 90/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8525 - val_loss: 0.3440 - val_accuracy: 0.9000\n",
            "Epoch 91/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8500 - val_loss: 0.3427 - val_accuracy: 0.9000\n",
            "Epoch 92/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8525 - val_loss: 0.3415 - val_accuracy: 0.9000\n",
            "Epoch 93/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8512 - val_loss: 0.3405 - val_accuracy: 0.9000\n",
            "Epoch 94/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8500 - val_loss: 0.3397 - val_accuracy: 0.9000\n",
            "Epoch 95/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8512 - val_loss: 0.3386 - val_accuracy: 0.9000\n",
            "Epoch 96/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8500 - val_loss: 0.3377 - val_accuracy: 0.9000\n",
            "Epoch 97/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8500 - val_loss: 0.3368 - val_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8512 - val_loss: 0.3358 - val_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8512 - val_loss: 0.3347 - val_accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8487 - val_loss: 0.3339 - val_accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONs2hF6V0kh",
        "colab_type": "text"
      },
      "source": [
        "###Logistic regression from scratch example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezy-C5dKV7Px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 2\n",
        "numpy_weights = np.ones(num_features+1)\n",
        "new_training_data = np.hstack((np.ones([training_size, 1]), training_data))\n",
        "\n",
        "\n",
        "def sigmoid(w, X):\n",
        "    z = X.dot(w)\n",
        "    return (1.0/(1 + np.exp(-z)))\n",
        "\n",
        "def gradient(w, X, y):\n",
        "    a = sigmoid(w, X)\n",
        "    return (a - y).T.dot(X)\n",
        "    \n",
        "\n",
        "def bce_loss(w, X, y):\n",
        "    prediction = sigmoid(w, X)\n",
        "    return -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
        "\n",
        "def train_weights(X, y, w, learning_rate, epochs):\n",
        "    for i in range(epochs):\n",
        "        cost = bce_loss(w, X, y)\n",
        "        w = w - learning_rate * gradient(w, X, y)\n",
        "\n",
        "    return w\n",
        "\n",
        "weights = train_weights(new_training_data, training_labels, numpy_weights, lr, num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ty7GE-zX2g",
        "colab_type": "text"
      },
      "source": [
        "#3. Building a model (for instance, here you can talk about the structure of a convent, what it's components are etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jShAM6QX0iEs",
        "colab_type": "text"
      },
      "source": [
        "A convolutional network (aka convnet) is a form of deep learning that is better suited for classification of images, since there is no simple way to distil meaningful data values that would be helpful in classification from an image. A convnet consists of two main sections: the convnet base, and the classifier.\n",
        "<br>\n",
        "<br>\n",
        "The classifier is made up of some number of densely connected layers that take the output of the convnet base and make a prediction for what classification the data should be under. This is what you would train most importantly if you were going to use a pretrained convnet base.\n",
        "<br>\n",
        "<br>\n",
        "The convnet base is the part of the model that distils computer readable data from an image. This is done by convolution/correlation. A number of filters are applied to every possible position of the image. These filters in the lowest levels of the base associate small patterns in pixels like diagonal lines or horizontal lines. As the data passes through more layers of the convnet base, the filters start piecing together more and more abstract ideas, like putting together two horizontal lines and two vertical lines all together makes a square. The lines would be the most basic layer of filters, and the next layer of filters would piece together the lines to see a square. the more filters the data passes through, the bigger of a concept can become, until it reaches the end and eventually goes through the classifier. \n",
        "<br>\n",
        "<br>\n",
        "One perhaps unintended effect of the filters is that using several filters in a layer will greatly increase the number of parameters in the model, making training take great amounts of time. One way to counter this effect is pooling. A common form of pooling is maxpooling. Maxpooling is where the number of parameters is condensed by only taking the max value of sub-sections of the image/filter layer. These sub-sections of the image are typically a 2-by-2 grid of pixels but can be any other size as well. These \"sub-sections\" are called windows. The result of maxpooling is a much smaller, more manageable number of parameters in the model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcI-zkNfZWkO",
        "colab_type": "text"
      },
      "source": [
        "##Visual aid for maxpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ_CydywwxPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of maxpooling:\n",
        "#  --------------------------------\n",
        "# |       |       ||       |       |                 window of size 2x2\n",
        "# |   4   |   1   ||   5   |   4   |                 image  of size 4x4\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   2   ||   4   |   2   |\n",
        "# |       |       ||       |       |\n",
        "# |===============||===============|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   6   |   6   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   1   |   0   |\n",
        "# |       |       ||       |       |\n",
        "#  --------------------------------\n",
        "#\n",
        "#  --------------------------------\n",
        "# |       |       ||       |       |\n",
        "# |  *4*  |   1   ||  *5*  |   4   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|            divided into 2x2 windows\n",
        "# |       |       ||       |       |            find the maximum in each window, then keep only that\n",
        "# |   1   |   2   ||   4   |   2   |\n",
        "# |       |       ||       |       |\n",
        "# |===============||===============|\n",
        "# |       |       ||       |       |\n",
        "# |  *1*  |   0   ||  *6*  |   6   |\n",
        "# |       |       ||       |       |\n",
        "# |-------|-------||-------|-------|\n",
        "# |       |       ||       |       |\n",
        "# |   1   |   0   ||   1   |   0   |\n",
        "# |       |       ||       |       |\n",
        "#  --------------------------------\n",
        "#\n",
        "#  --------------------------------\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |       4       ||       5       | \n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |===============||===============|\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |       1       ||       6       | \n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "# |               ||               |\n",
        "#  --------------------------------\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDTTmxiXmnj",
        "colab_type": "text"
      },
      "source": [
        "##Code examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YYqWGXKXq69",
        "colab_type": "text"
      },
      "source": [
        "###Convolution example code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-biQfsV8XhgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Error(Exception) : \n",
        "    \"\"\"Base class for exceptions in this module.\"\"\"\n",
        "\n",
        "class IncompatableMatrixAndKernel(Error) :\n",
        "    \"\"\"Exception raised when given a kernel larger than input_mat\"\"\"\n",
        "    def __init__(self, message):\n",
        "        self.message = message \n",
        "    pass\n",
        "\n",
        "class NonsquareMatrix(Error) :\n",
        "    \"\"\"Exception raised when given a non square matrix\"\"\"\n",
        "\n",
        "    def __init__(self, message):\n",
        "        self.message = message\n",
        "    pass\n",
        "\n",
        "\n",
        "# takes 2 2d numpy arrays input_mat and kernel_mat and multiplies element input_mat[x][y] with kernel_mat[x][y]\n",
        "# returns a 2d numpy array with the results\n",
        "def filter_mat(input_mat, kernel_mat):\n",
        "    result = np.empty((input_mat.shape[0], input_mat.shape[1]))\n",
        "\n",
        "    for i in range(input_mat.shape[0]):\n",
        "        for j in range(input_mat.shape[1]):\n",
        "            result[i][j] = input_mat[i][j] * kernel_mat[i][j]\n",
        "            \n",
        "    return result\n",
        "\n",
        "def conv2d(input_mat, kernel_mat):\n",
        "    if kernel_mat.shape[0] > input_mat.shape[0]:\n",
        "        raise IncompatableMatrixAndKernel(\"expected a kernel smaller than input_mat\")\n",
        "\n",
        "    if kernel_mat.shape[0] != kernel_mat.shape[1]:\n",
        "        raise NonsquareMatrix(\"expected a square kernel\")\n",
        "\n",
        "    if input_mat.shape[0] != input_mat.shape[1]:\n",
        "        raise NonsquareMatrix(\"expected a square input_mat\")\n",
        "\n",
        "    # comment above and uncomment below if an empty array is desired instead of an error\n",
        "    #if kernel_mat.shape[0] > input_mat.shape[0]:\n",
        "    #   return []\n",
        "    #\n",
        "    #if kernel_mat.shape[0] != kernel_mat.shape[1]:\n",
        "    #   return []\n",
        "    #\n",
        "    #if input_mat.shape[0] != input_mat.shape[1]:\n",
        "    #   return []\n",
        "\n",
        "    # flip the kernel 180 degrees\n",
        "    fliped_kernel_mat = np.rot90(kernel_mat, 2)\n",
        "   \n",
        "    # size of the kernel on the x and y axes\n",
        "    kernel_x = fliped_kernel_mat.shape[0]\n",
        "    kernel_y = fliped_kernel_mat.shape[1]\n",
        "\n",
        "    ## size of the kernel on the x and y axes\n",
        "    #kernel_x = kernel_mat.shape[0]\n",
        "    #kernel_y = kernel_mat.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_vgAGkWY0CV",
        "colab_type": "text"
      },
      "source": [
        "###Maxpooling example code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZW2mwz3Y5Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Error(Exception) : \n",
        "    \"\"\"Base class for exceptions in this module.\"\"\"\n",
        "\n",
        "class IncompatableMatrixAndWindow(Error) :\n",
        "    \"\"\"Exception raised when given a window size larger than the input_mat\"\"\"\n",
        "    def __init__(self, message):\n",
        "        self.message = message \n",
        "    pass\n",
        "\n",
        "class NonsquareMatrix(Error) :\n",
        "    \"\"\"Exception raised when given a non square matrix\"\"\"\n",
        "\n",
        "    def __init__(self, message):\n",
        "        self.message = message\n",
        "    pass\n",
        "\n",
        "def maxpooling2d(input_mat, s):\n",
        "    if s > input_mat.shape[0]:\n",
        "        raise IncompatableMatrixAndWindow(\"expected an s value smaller than the width of input_mat\")\n",
        "        \n",
        "    if input_mat.shape[0] != input_mat.shape[1]:\n",
        "        raise NonsquareMatrix(\"expected input_mat to have identical width and height\")\n",
        "     \n",
        "    # comment above and uncomment below if an empty array is desired instead of an error\n",
        "    #if s > input_mat.shape[0]:\n",
        "    #    return []\n",
        "    #    \n",
        "    #if input_mat.shape[0] != input_mat.shape[1]:\n",
        "    #    return []\n",
        "\n",
        "    #size of the output_mat on the x and y axes\n",
        "    output_mat_x = int(input_mat.shape[0] / s)\n",
        "    output_mat_y = int(input_mat.shape[1] / s)\n",
        "\n",
        "      \n",
        "    \n",
        "    # empty matrix to hold the results to return\n",
        "    output_mat = np.empty((output_mat_x, output_mat_y))\n",
        "\n",
        "    # while the kernel_mat is still in the bounds of the input_mat, slide the kernel down\n",
        "    for i in range(output_mat_x):\n",
        "        # while kernel_mat is still in the bounds of the input_mat, slide the kernel right\n",
        "        for j in range(output_mat_y):\n",
        "            # applies the kernel to the input_mat and stores the summation of the result to output_mat[i][j]\n",
        "            input_slice = input_mat[i*s:(i+1)*s,j*s:(j+1)*s]\n",
        "            output_mat[i][j] = np.amax(input_slice)\n",
        "    \n",
        "    return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDIGzOJAzXwV",
        "colab_type": "text"
      },
      "source": [
        "#4. Composing a model (for instance, you can talk here about learning rate etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zYfXjkw0ihz",
        "colab_type": "text"
      },
      "source": [
        "When training a machine learning model and updating the weights and biases to make its predictions better, a learning rate should be chosen. This learning rate determines how much each weight and bias should be changed each update to the model. If the learning rate is too high, the model might over correct every time the weights and biases are updated. If the learning rate is too low, then the model will take much longer to train, and might take too long to be practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kllv_bdzXpo",
        "colab_type": "text"
      },
      "source": [
        "#5. Training a model (for instance, you can talk about overfitting/underfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2wB4FNy0jEY",
        "colab_type": "text"
      },
      "source": [
        "When training a machine learning model, there are two extremes that should be avoided if possible: overfitting and underfitting.\n",
        "<br>\n",
        "<br>\n",
        "Overfitting occurs most typically when there is too little data or the model being used is too complicated. Overfitting is when a model trains on a set of data so well that it becomes less a less generalized model. The more the model trains on the training data, the model will start to lose the ability to predict outputs for inputs it has not seen, and will be more prone to fail edge cases.\n",
        "<br>\n",
        "<br>\n",
        "Underfitting is the opposite extreme, where either the model for the A.I. is too simple, or the model has not trained on enough data. An underfitted model will likely be less accurate than an overfitted model because an overfitted model can at least accurately predict the training data. Underfitted data will not be able to guess very well because the model would not have trained enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ibialF3zXZA",
        "colab_type": "text"
      },
      "source": [
        "#6. Finetuning a pretrained model (describe how you proceed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_NoCDF0j6u",
        "colab_type": "text"
      },
      "source": [
        "The process of finetuning a pretrained model has several steps. \n",
        "<br>\n",
        "<br>\n",
        "The first step is finding and downloading/importing a pre-trained model. Then be sure to freeze all of the chosen convnet model's layers so as to not lose the pretrained part of the model. After this, one must design a classifier to classify the output of the convnet base. Once this is done, the classifier must be attached to the convnet base in order to train the newly created classifier. Once the classifier has been trained sufficiently without overfitting, a small handful of shallow layers of the convnet base should be unfrozen and then the whole thing should be trained again in order to adapt the base to the classifier. Once training is finished, check that overfitting did not occur. After this, the various model parameters may be tweaked for possible performance improvement, model parameters being things like epochs, size of epochs, number of nodes, learning rate, etc. Other things to adjust could be trying to unfreeze different layers than the ones that were originally unfrozen and adding some data augmentation to your data pool for training. \n",
        "<br>\n",
        "<br>\n",
        "Data augmentation involves altering data so that it is almost like it is a different datum, but still classifies the same as the source. Ideally this data should still have very similar characteristics to its source, but just enough change to be different."
      ]
    }
  ]
}